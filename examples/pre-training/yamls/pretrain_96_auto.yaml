env:
    HOME: null

# ---------------------------model args-------------------------------------------------#
model_args:
    model_name_or_path: model_configs_auto/
    tokenizer_name: ./ernie/src/tokenizers/tokenizer_model
    output_dir: ./output/
    max_seq_length: 4096
    base_seq_length: 4096
    num_consecutive: 32
    sequence_parallel: 1
    enable_global_training_logs: False
    moe_use_aux_free_update_coef: 0.001
    global_logging_interval: 10
    model_config:
        moe_logging: True
        moe_use_aux_free: true
        multi_token_pred_depth: 0

# ---------------------------trainer args-------------------------------------------------#
trainer_args:
    input_dir: "0.4 ./demo_data/data-1-part0 0.6 ./demo_data/data-1-part0"
    split: "998,1,1"
    use_sp_callback: true
    moe_gate_lr_ratio: 0.01
    do_train: True
    dataloader_num_workers: 8
    prefetch_factor: 32
    overwrite_output_dir: 1
    disable_tqdm: 1
    logging_steps: 1
    eval_steps: 1000
    eval_iters: -1
    save_steps:  3000
    max_steps: 1000
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8
    learning_rate: 2.2e-4
    min_lr: 2.2e-5
    global_batch_size: 2
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    lr_scheduler: wsd:231084
    decay_function: 1-sqrt
    max_grad_norm: 1.0
    use_async_save: True
    weight_decay: 0.1
    warmup_steps: 200
    save_total_limit: 5
    bf16: True
    fp16_opt_level: "O2"
    scale_loss: 4096
    seed: 666
    pre_alloc_memory: 60

    tensor_parallel_degree: 4       # N7:8, N4:8, N1:4
    pipeline_parallel_degree: 2     # N7:7, N4:4, N1:2
    virtual_pp_degree: 1            # N7:8, N4:8, N1:1

    n_microbatches: 2
    pipeline_schedule_mode: "VPP"
    model_type: "ernie_pp"

    data_parallel_degree: 1
    sharding: "stage1"
    sharding_degree: 1
    amp_master_grad: 1
    pipeline_parallel_config: enable_delay_scale_loss
    sharding_parallel_config: split_param enable_fuse_optimizer_states
    sharding_comm_buffer_size_MB: 2048
    tensor_parallel_config: replace_with_parallel_cross_entropy

    skip_profile_timer: False
    ignore_data_skip: 0
    shuffle_consecutive: True
    load_sharded_model: True
    save_sharded_model: True
    ignore_load_lr_and_optim: False
    metrics_output_path: ./output/paddle_distributed_logs/

    use_moe: true
    moe_group: mp
    log_global_grad_norm: True
    enable_optimizer_timer: False
    gc_interval: 100000

    enable_auto_parallel: 1
    to_static: 0
